{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the Naturalness of Software Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openai in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from openai) (3.8.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from requests>=2.20->openai) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from requests>=2.20->openai) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (1.26.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: jdc in c:\\users\\haotian\\.conda\\envs\\lm\\lib\\site-packages (0.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade tiktoken\n",
    "%pip install --upgrade openai\n",
    "%pip install numpy \n",
    "%pip install jdc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The experiment is built on top of the following statistical formulas:\n",
    "\n",
    "- The language model is based on the n-gram theory:\n",
    "$$p(a_4|a_1a_2a_3) = \\frac{\\text{count}(a_1a_2a_3a_4)}{\\text{count}(a_1a_2a_3*)}$$\n",
    "\n",
    "- These models are estimated on a corpus using maximum likelihood based frequency-counting of token sequences. Thus, if “∗” is a wildcard, we can estimate the probability that \\(a_4\\) follows the tokens \\(a_1\\), \\(a_2\\), \\(a_3\\) with:\n",
    "$$p(a_i|a_1...a_{i-1}) \\approx p(a_i|a_{i-3}a_{i-2}a_{i-1})$$\n",
    "\n",
    "- Validation: use cross-entropy to validate the quality of the language model. Cross-entropy is, in a sense, a measure of the \"surprise\" experienced by the model when it sees the actual data:\n",
    "$$H_M(s) = -\\frac{1}{n} \\sum_{i=1}^{n} \\log p_M(a_i|a_1...a_{i-1})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### necessary import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "import numpy as np\n",
    "import heapq \n",
    "import math\n",
    "import jdc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build a model object for the trigram language model\n",
    "Since the paper used trigram, I also used trigram for minimized token input and more efficient memory useage.\n",
    "##### variables:\n",
    "- `limit` is the number of suggestions that the model will provide. \n",
    "- `model_type` denotes what the model will be trained on (natural language, programing language)\n",
    "- `distribution` will store probability distribution of third token after each of the two token combination.\n",
    "- `train_set` and `experiment_set` will sepaprate training and experiment set for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trigram_model:\n",
    "    def __init__(self, limit=5, model_type=\"code\"):\n",
    "        self.limit = limit\n",
    "        self.type = model_type\n",
    "        self.distribution = {}\n",
    "        self.data = []  \n",
    "        self.train_set = []\n",
    "        self.experiment_set = []\n",
    "\n",
    "    def get_type(self):\n",
    "        \"\"\"\n",
    "        return the type of data the model is trained on\n",
    "        :return: type of data\n",
    "        \"\"\"\n",
    "        return self.type\n",
    "    def get_limit(self):\n",
    "        \"\"\"\n",
    "        return the maximum number of suggestions the model can provide\n",
    "        :return: suggestion limit \n",
    "        \"\"\"\n",
    "        return self.limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train() for training the model against a specific data set.\n",
    "for each two token combination, record the third token and track it's frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to trigram_model\n",
    "def train(self, data):\n",
    "    \"\"\"\n",
    "    Trains the model using the provided list of tokens.\n",
    "    :param data: List of tokens to train the model.\n",
    "    \"\"\"\n",
    "    self.data = data\n",
    "    unique_token_size = len(set(data))\n",
    "    total_size = len(data)\n",
    "    print(f\"the {self.get_type()} model is trained based on {total_size} token(s)\")\n",
    "    print(f\"the {self.get_type()} model is trained based on {unique_token_size} unique token(s)\")\n",
    "    experiment_size = int(total_size * 0.1)  # Size of the experimental set (10% of the total)\n",
    "\n",
    "    # To ensure that the experiment set is a continuous chunk, we select a random starting index\n",
    "    start_index = np.random.randint(0, total_size - experiment_size)\n",
    "\n",
    "    # Define the end index for the experiment set\n",
    "    end_index = start_index + experiment_size\n",
    "\n",
    "    # Split the original data into training and experiment sets while maintaining continuity\n",
    "    self.experiment_set = data[start_index:end_index]\n",
    "    self.train_set = data[:start_index] + data[end_index:]\n",
    "\n",
    "    # Now, you can proceed with the training on the self.train_set\n",
    "    for i in range(len(self.train_set) - 2):\n",
    "        # Create the trigram parts\n",
    "        token1, token2, token3 = self.train_set[i], self.train_set[i + 1], self.train_set[i + 2]\n",
    "        key = (token1, token2)\n",
    "\n",
    "        # If the key already exists in the distribution, update the frequency of the third token\n",
    "        if key in self.distribution:\n",
    "            if token3 in self.distribution[key]:\n",
    "                self.distribution[key][token3] += 1  # Increment the count for the existing third token\n",
    "            else:\n",
    "                self.distribution[key][token3] = 1  # Initialize the count for the new third token\n",
    "        else:\n",
    "            # If the key doesn't exist, create a new entry in the distribution\n",
    "            self.distribution[key] = {token3: 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### deterministic_train() do the same thing except that the training set and experiment set are fixed for each training iteration. use this method to stably test the model's reliability with changing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to trigram_model\n",
    "def deterministic_train(self, data):\n",
    "    \"\"\"\n",
    "    Trains the model using the provided list of tokens.\n",
    "    :param data: List of tokens to train the model.\n",
    "    \"\"\"\n",
    "    self.data = data\n",
    "    unique_token_size = len(set(data))\n",
    "    total_size = len(data)\n",
    "    print(f\"the {self.get_type()} model is trained based on {total_size} token(s)\")\n",
    "    print(f\"the {self.get_type()} model is trained based on {unique_token_size} unique token(s)\")\n",
    "    experiment_size = int(total_size * 0.1)  # Size of the experimental set (10% of the total)\n",
    "    # Split the original data into training and experiment sets while maintaining continuity\n",
    "    self.experiment_set = data[:experiment_size]\n",
    "    self.train_set = data[experiment_size:]\n",
    "\n",
    "    # Now, you can proceed with the training on the self.train_set\n",
    "    for i in range(len(self.train_set) - 2):\n",
    "        # Create the trigram parts\n",
    "        token1, token2, token3 = self.train_set[i], self.train_set[i + 1], self.train_set[i + 2]\n",
    "        key = (token1, token2)\n",
    "\n",
    "        # If the key already exists in the distribution, update the frequency of the third token\n",
    "        if key in self.distribution:\n",
    "            if token3 in self.distribution[key]:\n",
    "                self.distribution[key][token3] += 1  # Increment the count for the existing third token\n",
    "            else:\n",
    "                self.distribution[key][token3] = 1  # Initialize the count for the new third token\n",
    "        else:\n",
    "            # If the key doesn't exist, create a new entry in the distribution\n",
    "            self.distribution[key] = {token3: 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### re-train the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to trigram_model\n",
    "def retrain(self):\n",
    "    \"\"\"\n",
    "    Retrains the model with the original training data.\n",
    "    the training and experiment set will be randomized\n",
    "    \"\"\"\n",
    "    if self.data:\n",
    "        self.train(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make prediction on given tokens. only give up to `limit` amount of suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to trigram_model\n",
    "def predict(self, tokens):\n",
    "        \"\"\"\n",
    "        Predicts the next set of tokens based on the input.\n",
    "        :param tokens: List of tokens to base the prediction on.\n",
    "        :return: A list of predicted tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(tokens, tuple) or len(tokens) != 2:\n",
    "            raise ValueError(\"Input must be a tuple of exactly two tokens.\")\n",
    "\n",
    "        # Check if the token pair is in the distribution.\n",
    "        if tuple(tokens) not in self.distribution:\n",
    "            # print(\"Token sequence not found in the training data.\")\n",
    "            return []\n",
    "\n",
    "        # Get all possible continuations and their frequencies.\n",
    "        possible_tokens = self.distribution[tuple(tokens)]\n",
    "\n",
    "        # If there are fewer continuations than the limit, return all of them.\n",
    "        if len(possible_tokens) <= self.limit:\n",
    "            return list(possible_tokens.keys())\n",
    "\n",
    "        # Otherwise, we need to extract the 'self.limit' most frequent continuations.\n",
    "        # 'heapq.nlargest' helps efficiently find the largest elements in a collection.\n",
    "        # We use a lambda function to specify that we're comparing the values (frequencies) in the dictionary.\n",
    "        most_frequent_tokens = heapq.nlargest(self.limit, possible_tokens, key=lambda x: possible_tokens[x])\n",
    "        return most_frequent_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate self entropy and cross entropy. self entropy will be calculated from experiment set; cross entropy will be based on provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to trigram_model\n",
    "def calculate_self_entropy(self):\n",
    "        \"\"\"\n",
    "        Calculates the cross-entropy for the experiment set based on the trigram model.\n",
    "        :return: The cross-entropy value.\n",
    "        \"\"\"\n",
    "        n = len(self.experiment_set) - 2  # since we're working with trigrams\n",
    "        if n <= 0:\n",
    "            return 0  # Avoid division by zero or taking log of zero. Handle the edge case.\n",
    "\n",
    "        total_log_probability = 0.0\n",
    "\n",
    "        # Iterate through the experiment set with a step of 3 since it's a trigram model\n",
    "        for i in range(n):\n",
    "            # Extract trigram\n",
    "            a1, a2, a3 = self.experiment_set[i], self.experiment_set[i+1], self.experiment_set[i+2]\n",
    "            \n",
    "            # Fetch the conditional probability of the trigram from the distribution\n",
    "            bigram_prob = self.distribution.get((a1, a2), {})\n",
    "            trigram_prob = bigram_prob.get(a3, 0)\n",
    "            \n",
    "            # To get conditional probability, we need to normalize by the sum of all possibilities for the bigram\n",
    "            conditional_prob = trigram_prob / (sum(bigram_prob.values()) or 1)\n",
    "            \n",
    "            # The provided formula uses log base e. If the probability is 0, it's undefined, so we skip it.\n",
    "            if conditional_prob > 0:\n",
    "                total_log_probability += math.log(conditional_prob)\n",
    "\n",
    "        # Compute the cross-entropy\n",
    "        entropy = - total_log_probability / n\n",
    "        return entropy\n",
    "\n",
    "def calculate_cross_entropy(self,data):\n",
    "    \"\"\"\n",
    "    Predicts the next set of tokens based on the input.\n",
    "    :param tokens: List of tokens to base the prediction on.\n",
    "    :return: A list of predicted tokens.\n",
    "    \"\"\"\n",
    "    # Determine the length for 10% of the data\n",
    "    ten_percent_length = len(data) // 10\n",
    "    if ten_percent_length <= 2:\n",
    "        return 0  # Avoid taking log of zero and ensure we have at least one trigram. Handle the edge case.\n",
    "\n",
    "    # Randomly select a starting point for the 10% segment\n",
    "    start_idx = np.random.randint(0, len(data) - ten_percent_length + 1)\n",
    "    subset = data[start_idx:start_idx + ten_percent_length]\n",
    "\n",
    "    n = len(subset) - 2  # since we're working with trigrams\n",
    "    total_log_probability = 0.0\n",
    "\n",
    "    # Iterate through the subset with a step of 1 since it's a trigram model\n",
    "    for i in range(n):\n",
    "        # Extract trigram\n",
    "        a1, a2, a3 = subset[i], subset[i + 1], subset[i + 2]\n",
    "\n",
    "        # Fetch the conditional probability of the trigram from the distribution\n",
    "        bigram_prob = self.distribution.get((a1, a2), {})\n",
    "        trigram_prob = bigram_prob.get(a3, 0)\n",
    "\n",
    "        # To get conditional probability, we need to normalize by the sum of all possibilities for the bigram\n",
    "        conditional_prob = trigram_prob / (sum(bigram_prob.values()) or 1)\n",
    "\n",
    "        # The provided formula uses log base e. If the probability is 0, it's undefined, so we skip it.\n",
    "        if conditional_prob > 0:\n",
    "            total_log_probability += math.log(conditional_prob)\n",
    "\n",
    "    # Calculate the cross-entropy according to the formula\n",
    "    cross_entropy = -total_log_probability / n\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate average accuracy of the model. the avg accuracy is calculated by making prediction on each of the two token combinations in experiment set and if the provided suggestions include the correct answer, it is considered successful and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to trigram_model\n",
    "def calculate_avg_accuracy(self):\n",
    "    \"\"\"\n",
    "    Calculate the average prediction accuracy based on the experiment_set.\n",
    "    :return: The average accuracy as a float.\n",
    "    \"\"\"\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Iterate over the experiment_set with an index\n",
    "    for i in range(len(self.experiment_set) - 2):\n",
    "        # Get the actual tokens\n",
    "        actual_tokens = self.experiment_set[i:i+3]\n",
    "        \n",
    "        # Get the predicted tokens using the first two tokens\n",
    "        predicted_tokens = self.predict(tuple(actual_tokens[:2]))\n",
    "        \n",
    "        # Check if the third actual token is in the predicted tokens\n",
    "        if actual_tokens[2] in predicted_tokens:\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        total_predictions += 1\n",
    "    \n",
    "    # Return the average accuracy\n",
    "    return correct_predictions / total_predictions if total_predictions > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utilize openai's tokenizer to convert raw text into token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(file_path):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Read and print each line\n",
    "        for line in file:\n",
    "            data.extend(encoding.encode(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenize the dataset and feed them into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the natural language model is trained based on 2599953 token(s)\n",
      "the natural language model is trained based on 31955 unique token(s)\n",
      "the programming language model is trained based on 237536 token(s)\n",
      "the programming language model is trained based on 7499 unique token(s)\n"
     ]
    }
   ],
   "source": [
    "english_file_path = \"data/english.txt\"\n",
    "python_file_path = \"data/Python_code_data.txt\"\n",
    "\n",
    "model_eng = trigram_model(limit = 3, model_type = \"natural language\")\n",
    "model_py = trigram_model(limit = 3, model_type = \"programming language\")\n",
    "data1 = tokenize(english_file_path)\n",
    "data2 = tokenize(python_file_path)\n",
    "model_eng.deterministic_train(data1)\n",
    "model_py.deterministic_train(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the model and gather insights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_entropy of model training on english:  1.2718418231084239\n",
      "self_entropy of model training on python:  1.0818544238227277\n",
      "cross_entropy of model training on english:  0.1272424575200946\n",
      "cross_entropy of model training on python:  0.0023872427918150514\n",
      "avg accuracy of model training on english:  0.25172985426530714\n",
      "avg accuracy of model training on python:  0.60923750578923\n"
     ]
    }
   ],
   "source": [
    "self_entropy1 = model_eng.calculate_self_entropy()\n",
    "avg_accuracy1 = model_eng.calculate_avg_accuracy()\n",
    "cross_entropy1 = model_eng.calculate_cross_entropy(data2)\n",
    "\n",
    "self_entropy2 = model_py.calculate_self_entropy()\n",
    "avg_accuracy2 = model_py.calculate_avg_accuracy()\n",
    "cross_entropy2 = model_py.calculate_cross_entropy(data1)\n",
    "\n",
    "print(\"self_entropy of model training on english: \", self_entropy1)\n",
    "print(\"self_entropy of model training on python: \", self_entropy2)\n",
    "print(\"cross_entropy of model training on english: \", cross_entropy1)\n",
    "print(\"cross_entropy of model training on python: \", cross_entropy2)\n",
    "print(\"avg accuracy of model training on english: \", avg_accuracy1)\n",
    "print(\"avg accuracy of model training on python: \", avg_accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### challenges along the journey\n",
    "1. Dealing with unknown token\n",
    "- problem can be resolvewd with Kneser-Ney algorithm and Laplace smoothing\n",
    "- for simplicity I ignore the unseen token\n",
    "2. Data collection\n",
    "- I searched from github, online libraries for data\n",
    "- programmatically parse necessary data from raw data in different sources.\n",
    "- Available dataset are too big to train on. \n",
    "3. tokenizer\n",
    "- I want to write my own tokenizer but parsing code token it's much more complicated then I thought\n",
    "- Then I used openai's tokenizer for parsing code\n",
    "4. shortage of dataset\n",
    "- due to shortage of data, its hard to perform cross entropy\n",
    "5. The data I collected is too far from the experiment.\n",
    "- Although the accuracy of my prediction is even better than what the paper has proposed, the entropy I calculated does not align with their's"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
